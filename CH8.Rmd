---
title: "CH 8: Least Squares and Maximum Likelihood"
output: pdf_document
---

\renewcommand{\vec}[1]{\mathbf{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 3, fig.width = 5, fig.align = 'center')
library(tidyverse) 
library(gridExtra)
library(rstanarm)
library(arm)
set.seed(10132020)
```


### Least Squares

Least squares is a common method for estimating regression coefficients in a linear model. Consider the model, $y_i = \beta_0 + \beta_1 x_i + \epsilon_i,$ where $i$ indexes the $i^{th}$ observation.

\vfill


\vfill



\vfill

```{r, echo = F}
tibble(x = runif(30, -1, 1)) %>% mutate(y = x + rnorm(30)) %>%
  ggplot(aes(y = y, x = x)) + geom_point() + theme_void()
```

\vfill

\newpage

The residual sum of square are formally defined as:

$$RSS = \sum_{i=1}^{n} \left( y_i - \hat{\beta_0} - \hat{\beta_1}x_i\right)^2$$

\vfill

Hence, to minimize this value, we will take the derivatives with respect to $\beta_0$ and $\beta_1$

\vfill
\vfill
\vfill
\vfill

\newpage

Then we also solve for $\beta_1$ plugging in the value of $\beta_0$

\vfill

\begin{align*}
\frac{d RSS}{d \beta_1} & = - 2 \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i\right)x_i\\
\text{(set = 0)\;\;} - 2 \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i\right)x_i  & = 0\\
\sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1x_i\right) x_i & = 0\\
\sum_{i=1}^{n}x_i y_i - \sum_{i=1}^{n} x_i \beta_0 - \sum_{i=1}^{n} \beta_1 x_i^2 & = 0\\
\sum_{i=1}^{n}x_i y_i - \sum_{i=1}^{n} x_i (\bar{y} - \beta_1 \bar{x}) - \sum_{i=1}^{n} \beta_1 x_i^2 & = 0\\
\sum_{i=1}^{n}x_i y_i - \bar{y} \sum_{i=1}^{n} x_i  - \beta_1 \bar{x} \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 & = 0\\
\beta_1 \left( \sum_{i=1}^{n}x_i^2 - \bar{x} \sum_{i=1}^{n} x_i \right) & = \sum_{i=1}^{n}x_i y_i - \bar{y} \sum_{i=1}^{n} x_i\\
\beta_1 \left( \sum_{i=1}^{n}x_i^2 - \bar{x} \sum_{i=1}^{n} x_i \right) & = \sum_{i=1}^{n}x_i y_i - \sum_{i=1}^{n} \frac{y_i}{n}  n\bar{x} \\
\beta_1 \left( \sum_{i=1}^{n}x_i^2 - \bar{x} \sum_{i=1}^{n} x_i \right) & = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i \\
\beta_1 \left( \sum_{i=1}^{n}x_i^2 - n\bar{x}^2  \right) & = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i \\
.& = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i\\
.& = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i\\
.& = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i\\
\beta_1  \sum_{i=1}^{n} \left(x_i - \bar{x} \right) ^2   & = \sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i \\
\beta_1 & = \frac{\sum_{i=1}^{n} \left( x_i - \bar{x} \right) y_i}  {\sum_{i=1}^{n} \left(x_i - \bar{x} \right) ^2}
\end{align*}

\newpage

This calculation is actually considerably easier using matrix notation.

\vfill

Let the model be specified as $\vec{y} = X \vec{\beta} + \vec{\epsilon}$, 
where 

- $\vec{y}$ is a $n \times 1$ matrix (or vector), such that 

\vfill

- $X$ is a $n \times 2$ matrix, 



\vfill

- $\vec{\beta}$ is $2 \times 1$ matrix, 


\vfill

- $\vec{\epsilon}$ is a $n \times 1$ matrix (or vector), such that 

\vfill

To verify this matrix algebra, we can look at the first row of the matrix which results in 

\vfill
\newpage

The residuals can be defined as

$$\vec{r} = \vec{y} - X \hat{\vec{\beta}} = 
\begin{pmatrix}
y_1 - \hat{\beta_0} - \hat{\beta_1} x_1 \\
y_2 - \hat{\beta_0} - \hat{\beta_1} x_2 \\
\vdots \\
y_n - \hat{\beta_0} - \hat{\beta_1} x_n 
\end{pmatrix}$$

\vfill

Then the sum of squares is written as:
$$\sum_{i=1}^n = \left(y_i - \hat{\beta_0} - \hat{\beta_1} x_i \right)^2 = \vec{r}^T \vec{r}$$
\vfill

To minimize the sum of squares of the residuals, consider

\begin{align*}
\vec{r}^T \vec{r} = &(\vec{y} - X \vec{\beta})^T(\vec{y} - X \vec{\beta})\\ = &\vec{y}^T\vec{y} - \vec{y}T X \vec{\beta} - \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta}\\
= &\vec{y}^T\vec{y} -2 \vec{\beta}^T X^T \vec{y} + \vec{\beta}^T X^T X \vec{\beta}
\end{align*}

\vfill

Now take the derivative

\begin{align}
\frac{d \; SSR}{d\; \vec{\beta}^T} = -2 X^t \vec{y} + 2 X^T X \vec{\beta} \end{align}

\vfill
and set = 0
\begin{align}
0 & -2 X^T \vec{y} + 2 X^T X \vec{\beta} \\
X^T \vec{y} & =  X^T X \vec{\beta}\\
(X^T X)^{-1} X^T \vec{y} = \vec{\beta}
\end{align}

\vfill

$\sigma$ is also estimated with the sum of the squared residuals

$$\hat{\sigma} = \sqrt{\frac{1}{n-k} \sum_{i=1}^n (y_i - X_i \hat{\vec{\beta}})^2}$$,

where $k$ is the number of predictors in the model (including the intercept)

\newpage

With least-squares estimation, there is no specification of a probability distribution. This is strictly a geometric procedure. 


\vfill

Recall a regression model can be written as

$$y = \beta_0 + \beta_1 x + \epsilon, \; \epsilon \sim N(0, \sigma^2)$$

or

\vfill

Hence, the likelihood corresponds to how well the data point $y$ corresponds to the normal density (or likelihood),

\vfill

### lm and stan_glm

With Bayesian inference, and the use of prior information, the posterior is a product of the the likelihood of the data as well as the prior.

\vfill

The role prior distribution is often characterized as a penalty (to the likelihood) or regularization, where the prior can down weight some values of the parameter.

\vfill

As we have seen, Bayesian inference enables simulated based approaches for summarizing parameter coefficients, contrasts, and predictions. Bayesian inference also allows uncertainty to be expressed using probability, as opposed to using confidence. However, the cost is specifying a prior, which can be subjective. 

\vfill

\newpage

Given that maximum-likelihood methods are optimizaing the likelihood while Bayesian inference focus on the posterior (likelihood + prior), we'd expect differences in the results.

\vfill

The default priors are weakly informative, so that posterior is not vastly different from the likelihood.

\vfill

```{r}
beer <- read_csv('http://math.montana.edu/ahoegh/Data/Brazil_cerveja.csv')
beer %>% lm(consumed ~ max_tmp, data = .) %>% coef()
beer %>% 
  stan_glm(consumed ~ max_tmp, data = ., refresh = 0, iter = 100000) %>% coef()
```

\vfill

We can explicitly state that flat (uniform, uniformative) priors, so that the posterior and the likelihood are are the same

```{r}
beer <- read_csv('http://math.montana.edu/ahoegh/Data/Brazil_cerveja.csv')
beer %>% lm(consumed ~ max_tmp, data = .) %>% coef()
beer %>% 
  stan_glm(consumed ~ max_tmp, data = .,
           refresh = 0, iter = 100000,
           prior_intercept = NULL,
           prior = NULL,
           prior_aux = NULL) %>% coef()
```

### Uncertainty Intervals

\vfill

\vfill

In general, if you use classical methods and confidence intervals or Bayesian methods and credible intervals, it is important to be transparent about the model fitting process and interpretations.

\vfill

I will be more lenient about this, but outside of my class, make sure you are clearly articulating the type of estimation and appropriate interpretation.

